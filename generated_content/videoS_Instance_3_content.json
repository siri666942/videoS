{
  "title": "Transformer架构原理详解",
  "summary": "本文详细解析了Transformer架构的核心原理。首先介绍了词嵌入(Embedding)的概念，通过将文本切分为Token并映射到高维向量空间，使语义相近的词在空间中距离更近。重点阐述了注意力机制的三要素：Q(查询)、K(键)、V(值)，通过点积计算词与词之间的关注程度，再经过缩放(Scale)和SoftMax转换得到注意力权重。多头注意力机制通过并行计算多个注意力头来捕捉不同的语义关系。编码器由多头注意力、残差连接、归一化和前馈网络组成，逐层加深对文本的理解。解码器则通过自注意力和交叉注意力，结合编码器的输出逐步生成目标语言。最后说明Transformer的两种常见架构：编码器-解码器完整架构适用于翻译任务，而纯编码器或纯解码器架构分别用于理解或生成任务。",
  "mindmap": "# Transformer架构原理\n\n## 文本数字化处理\n- 文本需转换成数字\n- 简单方法：Unicode编码\n- 问题：编号无法承载语义信息\n\n## 词嵌入（Embedding）\n- 将文本切分成Token\n- 构建多维坐标系表示语义\n- 语义相近的词在空间中靠近\n- 通过向量表示每个Token\n- 加入位置信息\n\n## QKV机制\n- 查询键（Key）：词块本身\n- 值（Value）：词块背后的含义\n- 查询（Query）：词块想关注什么\n- 通过三个权重矩阵变换得到Q、K、V\n\n## 注意力机制\n- 每个Token的Q与所有Token的K计算点积\n- Scale：防止点积数值过大\n- SoftMax：转换为注意力权重\n- 权重乘以V得到融合上下文后的表示\n\n## 多头注意力\n- 并行计算多次注意力\n- 关注不同侧重点\n- 结果拼接在一起\n\n## 编码器结构\n- 残差连接（Add）\n- 归一化（Norm）\n- 前馈网络（FFN）\n- 多个编码层堆叠\n\n## 解码器结构\n- 使用BOS符号开始翻译\n- 结合编码器输出预测下一个词\n- 包含多头自注意力和交叉注意力\n- 多个解码层堆叠\n\n## Transformer应用变体\n- 编码器+解码器：经典架构\n- 纯编码器：如BERT，用于理解任务\n- 纯解码器：如GPT，用于生成任务",
  "quiz": {
    "questions": [
      {
        "id": 1,
        "question": "在Transformer中，将文本转换为数字表示时，为什么要使用Embedding而不是简单的Unicode编码？",
        "options": [
          "Embedding能够包含语义信息，而Unicode编码只是单纯的数字编号",
          "Embedding比Unicode编码占用更少的内存空间",
          "Unicode编码处理中文文本时存在兼容性问题",
          "Embedding的计算速度比Unicode编码更快"
        ],
        "correct_answer": 0,
        "explanation": "Unicode编码只是将字符映射为数字编号，这些数字之间没有任何语义关系。而Embedding通过多维坐标系将词语表示为向量，相似含义的词在向量空间中距离更近，能够有效捕捉语义信息。",
        "difficulty": "medium"
      },
      {
        "id": 2,
        "question": "在Transformer的注意力机制中，Q、K、V分别代表什么？",
        "options": [
          "Q代表查询，K代表键，V代表值",
          "Q代表质量，K代表知识，V代表价值",
          "Q代表问题，K代表答案，V代表验证",
          "Q代表快速，K代表关键，V代表变量"
        ],
        "correct_answer": 0,
        "explanation": "Q(Query)代表当前Token想要关注什么，K(Key)代表其他Token能提供什么信息，V(Value)代表Token背后的真正含义。通过Q与K的点积计算相关性，然后用相关性权重对V进行加权求和。",
        "difficulty": "hard"
      },
      {
        "id": 3,
        "question": "Transformer架构中的多头注意力机制的主要作用是什么？",
        "options": [
          "从多个角度捕捉词语之间的复杂关系",
          "提高模型的训练速度",
          "减少内存使用量",
          "简化模型的计算复杂度"
        ],
        "correct_answer": 0,
        "explanation": "多头注意力通过并行计算多个注意力头，每个头关注不同类型的关系，如语义关联、语法结构依赖等，然后将结果拼接起来，从而更全面地捕捉词语间的复杂关系。",
        "difficulty": "hard"
      },
      {
        "id": 4,
        "question": "在翻译任务中，Transformer的解码器使用掩码多头自注意力的主要目的是什么？",
        "options": [
          "确保生成过程从左到右，当前词只依赖前面已生成的词",
          "提高翻译的准确率",
          "减少计算资源的消耗",
          "避免过拟合现象"
        ],
        "correct_answer": 0,
        "explanation": "掩码多头自注意力通过遮盖后面未生成的词，确保在预测第N个词时只能看到前N-1个词，这样保证了生成过程的顺序性和合理性。",
        "difficulty": "medium"
      }
    ]
  }
}